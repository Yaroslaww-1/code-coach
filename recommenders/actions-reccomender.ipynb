{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f72dc583-ef61-4132-af77-0657692abf67",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Custom Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97a5f3-aeb6-4cf4-b016-67bc0c230c62",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Use Neural Collaborative Filtering to create movie recommendations*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5208b5-ab70-47af-84e1-3f1253621967",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Make sure you are using the following SageMaker configurations:\n",
    " - Kernel: Python 3.8, Tensorflow 2.6 CPU \n",
    " - Instance Type: ml.m5.large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab44e52-e6c8-4226-a22a-0c98a861de2d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Run all\n",
    "\n",
    " - If you are in a SageMaker Notebook instance, you can go to the **Cell** tab and choose **Run All**\n",
    " - If you are in SageMaker Studio, you can go to the **Run** tab and choose **Run All Cells**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98679ed3-4b52-4fc3-8209-91e0c9359c1a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Contents\n",
    "1. Background\n",
    "2. Data preparation\n",
    "3. NCF network in TensorFlow 2.0\n",
    "4. Perform model training using script mode\n",
    "5. Deploy the trained model using Amazon SageMaker hosting services as an endpoint\n",
    "6. Run inference using the model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bb9d0-7f55-4e23-9114-99544ae0224f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d26873-0e5b-4f4f-a6c9-608cb6083304",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This solution relies on a configuration file to run the provisioned AWS resources. Run the following cells to generate the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef2576c-c920-4dec-aea5-648550c376d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5057091-a6f6-412f-9bbe-d3a16a2463c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client = boto3.client('servicecatalog')\n",
    "cwd = os.getcwd().split('/')\n",
    "i = cwd.index('S3Downloads')\n",
    "pp_name = cwd[i + 1]\n",
    "pp = client.describe_provisioned_product(Name=pp_name)\n",
    "record_id = pp['ProvisionedProductDetail']['LastSuccessfulProvisioningRecordId']\n",
    "record = client.describe_record(Id=record_id)\n",
    "\n",
    "keys = [ x['OutputKey'] for x in record['RecordOutputs'] if 'OutputKey' and 'OutputValue' in x]\n",
    "values = [ x['OutputValue'] for x in record['RecordOutputs'] if 'OutputKey' and 'OutputValue' in x]\n",
    "stack_output = dict(zip(keys, values))\n",
    "\n",
    "with open(f'/root/S3Downloads/{pp_name}/stack_outputs.json', 'w') as f:\n",
    "    json.dump(stack_output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50fc0d50-cf14-453d-939a-bc0817060dd4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import json\n",
    "import sagemaker\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "session = sagemaker.Session()\n",
    "\n",
    "sagemaker_config = json.load(open(\"stack_outputs.json\"))\n",
    "role = sagemaker.get_execution_role()\n",
    "solution_bucket = sagemaker_config[\"SolutionS3Bucket\"]\n",
    "region = sagemaker_config[\"AWSRegion\"]\n",
    "library_version = sagemaker_config[\"LibraryVersion\"]\n",
    "solution_name = sagemaker_config[\"SolutionName\"]\n",
    "bucket = sagemaker_config[\"S3Bucket\"]\n",
    "endpoint_name = sagemaker_config[\"SolutionPrefix\"] + \"-ncf-endpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c307b91-8da2-4e75-a4d8-c0509fda2f7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Background\n",
    "*This notebook is based on the [Building a customized recommender system in Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/building-a-customized-recommender-system-in-amazon-sagemaker/) blog post.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d139f303-c802-499d-91a7-4305e99567ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Recommender systems](https://en.wikipedia.org/wiki/Recommender_system) help you tailor customer experiences on online platforms. [Amazon Personalize](https://aws.amazon.com/personalize/) is an artificial intelligence and machine learning service that specializes in developing recommender system solutions. It automatically examines data, performs feature and algorithm selection, optimizes models based on data, and deploys and hosts models for real-time recommendation inference. However, if you need to access the weights for a trained model, you may need to build your recommender system from scratch. Use this solution to train and deploy a customized recommender system in TensorFlow 2.0, using a [Neural Collaborative Filtering](https://arxiv.org/abs/1708.05031) (NCF) (He et al., 2017) model on [Amazon SageMaker](https://aws.amazon.com/sagemaker/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe2fd7-663f-4885-bac9-169792c151a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Understanding Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb51f13-9a1b-4ca2-81f5-1ef66021335a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A recommender system is a set of tools that helps provide users with a personalized experience by predicting user preference amongst a large number of options. Matrix factorization (MF) is a well-known approach to solving such a problem. Conventional MF solutions exploit explicit feedback in a linear fashion; explicit feedback consists of direct user preferences, such as ratings for movies on a five-star scale or binary preference on a product (like or not like). However, explicit feedback isn’t always present in datasets. NCF solves the absence of explicit feedback by only using implicit feedback, which is derived from user activity, such as clicks and views. In addition, NCF utilizes multi-layer perceptrons to introduce non-linearity into the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e612a84f-91c5-4053-842d-8235d5639a97",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Architecture overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e302e-56e1-401f-8253-d55acc0bca21",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "An NCF model contains two intrinsic sets of network layers: embedding and NCF layers. You use these layers to build a neural matrix factorization solution with two separate network architectures, generalized matrix factorization (GMF) and multi-layer perceptron (MLP), whose outputs are then concatenated as input for the final output layer. The following diagram from the original paper illustrates this architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836ee90-c766-4c22-ab0e-02b828232a4d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"docs/ncf-architecture.jpeg\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1948828-d10a-4540-9312-baac3dcaed31",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19035e0f-02a4-4794-9fc6-526014d73068",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This solution uses the MovieLens dataset. [MovieLens](https://grouplens.org/datasets/movielens/) is a movie rating dataset provided by GroupLens, a research lab at the University of Minnesota. Run the following cells to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433a1e7c-e8b1-4148-9a20-f4d8466bd49c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['s3://sagemaker-solutions-prod-eu-central-1/0.2.0/Customized-recommender-system/1.0.0/artifacts/dataset/README.txt',\n",
       " 's3://sagemaker-solutions-prod-eu-central-1/0.2.0/Customized-recommender-system/1.0.0/artifacts/dataset/inference.npy',\n",
       " 's3://sagemaker-solutions-prod-eu-central-1/0.2.0/Customized-recommender-system/1.0.0/artifacts/dataset/ratings.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "original_bucket = f\"s3://{solution_bucket}-{region}/{library_version}/{solution_name}\"\n",
    "original_data_prefix = f\"artifacts/dataset\"\n",
    "original_data = f\"{original_bucket}/{original_data_prefix}\"\n",
    "print(\"original data: \")\n",
    "S3Downloader.list(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f22d2f-67f8-4244-bc8f-27a5a642b434",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-solutions-prod-eu-central-1/0.2.0/Customized-recommender-system/1.0.0/artifacts/dataset/README.txt to data/README.txt\n",
      "download: s3://sagemaker-solutions-prod-eu-central-1/0.2.0/Customized-recommender-system/1.0.0/artifacts/dataset/inference.npy to data/inference.npy\n",
      "download: s3://sagemaker-solutions-prod-eu-central-1/0.2.0/Customized-recommender-system/1.0.0/artifacts/dataset/ratings.csv to data/ratings.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $original_data data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e5712-960c-45a5-9276-28a4da5332e6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47356fd1-f1ea-4b6f-8ab6-32fd45358913",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "=======\n",
      "\n",
      "This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from [MovieLens](http://movielens.org), a movie recommendation service. It contains 100836 ratings and 3683 tag applications across 9742 movies. These data were created by 610 users between March 29, 1996 and September 24, 2018. This dataset was generated on September 26, 2018.\n",
      "\n",
      "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
      "\n",
      "The data are contained in the files `links.csv`, `movies.csv`, `ratings.csv` and `tags.csv`. More details about the contents and use of all these files follows.\n",
      "\n",
      "This is a *development* dataset. As such, it may change over time and is not an appropriate dataset for shared research results. See available *benchmark* datasets if that is your intent.\n",
      "\n",
      "This and other GroupLens data sets are publicly available for download at .\n",
      "\n",
      "\n",
      "Usage License\n",
      "=============\n",
      "\n",
      "Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. The data set may be used for any research purposes under the following conditions:\n",
      "\n",
      "* The user may not state or imply any endorsement from the University of Minnesota or the GroupLens Research Group.\n",
      "* The user must acknowledge the use of the data set in publications resulting from the use of the data set (see below for citation information).\n",
      "* The user may redistribute the data set, including transformations, so long as it is distributed under these same license conditions.\n",
      "* The user may not use this information for any commercial or revenue-bearing purposes without first obtaining permission from a faculty member of the GroupLens Research Project at the University of Minnesota.\n",
      "* The executable software scripts are provided \"as is\" without warranty of any kind, either expressed or implied, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose. The entire risk as to the quality and performance of them is with you. Should the program prove defective, you assume the cost of all necessary servicing, repair or correction.\n",
      "\n",
      "In no event shall the University of Minnesota, its affiliates or employees be liable to you for any damages arising out of the use or inability to use these programs (including but not limited to loss of data or data being rendered inaccurate).\n",
      "\n",
      "Citation\n",
      "========\n",
      "\n",
      "To acknowledge use of the dataset in publications, please cite the following paper:\n",
      "\n",
      "> F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. \n",
      "\n",
      "\n",
      "Further Information About GroupLens\n",
      "===================================\n",
      "\n",
      "GroupLens is a research group in the Department of Computer Science and Engineering at the University of Minnesota. Since its inception in 1992, GroupLens's research projects have explored a variety of fields including:\n",
      "\n",
      "* recommender systems\n",
      "* online communities\n",
      "* mobile and ubiquitious technologies\n",
      "* digital libraries\n",
      "* local geographic information systems\n",
      "\n",
      "GroupLens Research operates a movie recommender based on collaborative filtering, MovieLens, which is the source of these data. We encourage you to visit  to try it out! If you have exciting ideas for experimental work to conduct on MovieLens, send us an email at  - we are always interested in working with external collaborators.\n",
      "\n",
      "\n",
      "Content and Use of Files\n",
      "========================\n",
      "\n",
      "Formatting and Encoding\n",
      "-----------------------\n",
      "\n",
      "The dataset files are written as [comma-separated values](http://en.wikipedia.org/wiki/Comma-separated_values) files with a single header row. Columns that contain commas (`,`) are escaped using double-quotes (`\"`). These files are encoded as UTF-8. If accented characters in movie titles or tag values (e.g. Misérables, Les (1995)) display incorrectly, make sure that any program reading the data, such as a text editor, terminal, or script, is configured for UTF-8.\n",
      "\n",
      "\n",
      "User Ids\n",
      "--------\n",
      "\n",
      "MovieLens users were selected at random for inclusion. Their ids have been anonymized. User ids are consistent between `ratings.csv` and `tags.csv` (i.e., the same id refers to the same user across the two files).\n",
      "\n",
      "Movie Ids\n",
      "---------\n",
      "\n",
      "Only movies with at least one rating or tag are included in the dataset. These movie ids are consistent with those used on the MovieLens web site (e.g., id `1` corresponds to the URL ). Movie ids are consistent between `ratings.csv`, `tags.csv`, `movies.csv`, and `links.csv` (i.e., the same id refers to the same movie across these four data files).\n",
      "\n",
      "\n",
      "Ratings Data File Structure (ratings.csv)\n",
      "-----------------------------------------\n",
      "\n",
      "All ratings are contained in the file `ratings.csv`. Each line of this file after the header row represents one rating of one movie by one user, and has the following format:\n",
      "\n",
      "    userId,movieId,rating,timestamp\n",
      "\n",
      "The lines within this file are ordered first by userId, then, within user, by movieId.\n",
      "\n",
      "Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
      "\n",
      "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
      "\n",
      "\n",
      "Tags Data File Structure (tags.csv)\n",
      "-----------------------------------\n",
      "\n",
      "All tags are contained in the file `tags.csv`. Each line of this file after the header row represents one tag applied to one movie by one user, and has the following format:\n",
      "\n",
      "    userId,movieId,tag,timestamp\n",
      "\n",
      "The lines within this file are ordered first by userId, then, within user, by movieId.\n",
      "\n",
      "Tags are user-generated metadata about movies. Each tag is typically a single word or short phrase. The meaning, value, and purpose of a particular tag is determined by each user.\n",
      "\n",
      "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
      "\n",
      "Movies Data File Structure (movies.csv)\n",
      "---------------------------------------\n",
      "\n",
      "Movie information is contained in the file `movies.csv`. Each line of this file after the header row represents one movie, and has the following format:\n",
      "\n",
      "    movieId,title,genres\n",
      "\n",
      "Movie titles are entered manually or imported from , and include the year of release in parentheses. Errors and inconsistencies may exist in these titles.\n",
      "\n",
      "Genres are a pipe-separated list, and are selected from the following:\n",
      "\n",
      "* Action\n",
      "* Adventure\n",
      "* Animation\n",
      "* Children's\n",
      "* Comedy\n",
      "* Crime\n",
      "* Documentary\n",
      "* Drama\n",
      "* Fantasy\n",
      "* Film-Noir\n",
      "* Horror\n",
      "* Musical\n",
      "* Mystery\n",
      "* Romance\n",
      "* Sci-Fi\n",
      "* Thriller\n",
      "* War\n",
      "* Western\n",
      "* (no genres listed)\n",
      "\n",
      "\n",
      "Links Data File Structure (links.csv)\n",
      "---------------------------------------\n",
      "\n",
      "Identifiers that can be used to link to other sources of movie data are contained in the file `links.csv`. Each line of this file after the header row represents one movie, and has the following format:\n",
      "\n",
      "    movieId,imdbId,tmdbId\n",
      "\n",
      "movieId is an identifier for movies used by . E.g., the movie Toy Story has the link .\n",
      "\n",
      "imdbId is an identifier for movies used by . E.g., the movie Toy Story has the link .\n",
      "\n",
      "tmdbId is an identifier for movies used by . E.g., the movie Toy Story has the link .\n",
      "\n",
      "Use of the resources listed above is subject to the terms of each provider.\n",
      "\n",
      "Cross-Validation\n",
      "----------------\n",
      "\n",
      "Prior versions of the MovieLens dataset included either pre-computed cross-folds or scripts to perform this computation. We no longer bundle either of these features with the dataset, since most modern toolkits provide this as a built-in feature. If you wish to learn about standard approaches to cross-fold computation in the context of recommender systems evaluation, see [LensKit](http://lenskit.org) for tools, documentation, and open-source code examples.\n"
     ]
    }
   ],
   "source": [
    "!cat data/README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f93776-4ade-447f-8568-1e958940a94d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The model for this solution mainly uses `ratings.csv`, which contains four columns:\n",
    "- `userId`\n",
    "- `movieId`\n",
    "- `rating`\n",
    "- `timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224dbd2b-c584-422f-8ada-2b4769f2efaf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "01277977-6d71-4152-b741-590261d966ca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb2067-cd4a-4cf8-a864-230d00d6ef5b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Read data and perform train and test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94417812-4706-4532-b6ab-c3367274ab9b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This solution uses `ratings.csv`, which contains explicit feedback data, as a proxy dataset to demonstrate the NCF solution. To fit this solution to your data, you need to define a metric to quantify a user rating and associate it with a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "59deba8e-3f98-4d82-b640-df741e6952f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read the rating data\n",
    "data_path = './data/actions.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "students_unique, df_students = np.unique(df['student_id'], return_inverse=True)\n",
    "df['student_id'] = df_students + 1\n",
    "\n",
    "coaches_unique, df_coaches = np.unique(df['coach_id'], return_inverse=True)\n",
    "df['coach_id'] = df_coaches + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "63b49b16-2401-4c5e-a2a4-3ca4ed493ae2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>coach_id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    student_id  coach_id  relevance\n",
       "0            1         1          1\n",
       "1            2         1          0\n",
       "2            3         1          5\n",
       "3            4         1          5\n",
       "4            1         2          0\n",
       "5            2         2          5\n",
       "6            3         2          0\n",
       "7            4         2          0\n",
       "8            1         3          5\n",
       "9            2         3          1\n",
       "10           3         3          0\n",
       "11           4         3          0\n",
       "12           1         4          5\n",
       "13           2         4          4\n",
       "14           3         4          0\n",
       "15           4         4          0\n",
       "16           1         5          0\n",
       "17           2         5          2\n",
       "18           3         5          0\n",
       "19           4         5          0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view part of the dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "624a6870-e068-4d45-ae63-133e82c74a5a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of hold out portion: 5\n"
     ]
    }
   ],
   "source": [
    "# figure out how to best divide the training and testing data\n",
    "max_holdout = df.groupby('student_id').coach_id.nunique().min()\n",
    "print(f'Maximum number of hold out portion: {max_holdout}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154bace-1041-4681-8a3b-a34cdddc473c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To perform a training and testing split, take the latest 10 items each user rated as the testing set and keep the rest as the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9704d77d-0386-4b71-81d5-5d1f9ffa6a5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(df: pd.DataFrame, holdout_num: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\" perform training/testing split\n",
    "    \n",
    "    @param df: dataframe\n",
    "    @param holdhout_num: number of items to be held out\n",
    "    \n",
    "    @return df_train: training data\n",
    "    @return df_test testing data\n",
    "    \n",
    "    \"\"\"\n",
    "    # first sort the data by time\n",
    "    df = df.sort_values(\n",
    "        by=['student_id'], \n",
    "        ascending=[True]\n",
    "    )\n",
    "    \n",
    "    # perform deep copy on the dataframe to avoid modification on the original dataframe\n",
    "    df_train = df.copy(deep=True)\n",
    "    df_test = df.copy(deep=True)\n",
    "    \n",
    "    # get test set\n",
    "    df_test = df_test.groupby(['student_id']) \\\n",
    "        .head(holdout_num) \\\n",
    "        .reset_index()\n",
    "    \n",
    "    # get train set\n",
    "    df_train = df_train.merge(\n",
    "        df_test[['student_id', 'coach_id']].assign(remove=1),\n",
    "        how='left'\n",
    "    ).query('remove != 1') \\\n",
    "        .drop('remove', 1) \\\n",
    "        .reset_index(drop=True)\n",
    "    \n",
    "    # sanity check to make sure we're not duplicating/losing data\n",
    "    assert len(df) == len(df_train) + len(df_test)\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b574b5fe-664e-4afd-96c4-4516a7cfd151",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df_train, df_test = train_test_split(df, holdout_num=1)\n",
    "# print(df_test)\n",
    "\n",
    "df_train = df.copy(deep=True)\n",
    "df_test = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf5e146-ee50-4d94-8133-393efab2d4a2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Perform negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec9aa3-299b-4109-8bd8-046e4c547057",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Because a user rating of an item must be a positive label, there are no negative samples in the dataset. Negative samples are needed for model training. Therefore, we randomly sample `n` items from the unseen movie list for every user and record a rating of `0` to provide negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9dae2c17-0cd7-497e-a83d-8b530fe4ead5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def negative_sampling(student_ids: list, coach_ids: list, items: list, n_neg: int) -> pd.DataFrame:\n",
    "    \"\"\"This function creates n_neg negative labels for every positive label\n",
    "    \n",
    "    @param student_ids: list of student ids\n",
    "    @param coach_ids: list of coach ids\n",
    "    @param items: unique list of coach ids\n",
    "    @param n_neg: number of negative labels to sample\n",
    "    \n",
    "    @return df_neg: negative sample dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    neg = []\n",
    "    ui_pairs = zip(student_ids, coach_ids)\n",
    "    records = set(ui_pairs)\n",
    "    \n",
    "    # for every positive label case\n",
    "    for (user_id, movie_id) in records:\n",
    "        # generate n_neg negative labels\n",
    "        for _ in range(n_neg):\n",
    "            # if the randomly sampled movie exists for that user\n",
    "            random_item = np.random.choice(items)\n",
    "            while(user_id, random_item) in records:\n",
    "                # resample\n",
    "                random_item = np.random.choice(items)\n",
    "            neg.append([user_id, random_item, 0])\n",
    "            \n",
    "    # convert to pandas dataframe for concatenation later\n",
    "    df_neg = pd.DataFrame(neg, columns=['student_id', 'coach_id', 'relevance'])\n",
    "    \n",
    "    return df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9debc9f1-9012-4aab-8d52-ee59805388db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    student_id  coach_id  relevance\n",
      "0            1         1          1\n",
      "1            2         1          0\n",
      "2            3         1          5\n",
      "3            4         1          5\n",
      "4            1         2          0\n",
      "5            2         2          5\n",
      "6            3         2          0\n",
      "7            4         2          0\n",
      "8            1         3          5\n",
      "9            2         3          1\n",
      "10           3         3          0\n",
      "11           4         3          0\n",
      "12           1         4          5\n",
      "13           2         4          4\n",
      "14           3         4          0\n",
      "15           4         4          0\n",
      "16           1         5          0\n",
      "17           2         5          2\n",
      "18           3         5          0\n",
      "19           4         5          0\n"
     ]
    }
   ],
   "source": [
    "# create negative samples for training set\n",
    "# neg_train = negative_sampling(\n",
    "#     student_ids=df_train.student_id.values, \n",
    "#     coach_ids=df_train.coach_id.values,\n",
    "#     items=df.coach_id.unique(),\n",
    "#     n_neg=5\n",
    "# )\n",
    "neg_train = df_train #TODO: remove\n",
    "print(neg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "034b08db-ff00-4bfb-8443-dbbf3cac2301",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created 20 negative samples\n"
     ]
    }
   ],
   "source": [
    "print(f'created {neg_train.shape[0]:,} negative samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "60a0e15e-3022-4030-877d-387c8740b193",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df_train = df_train[['student_id', 'coach_id']].assign(relevance=1)\n",
    "# df_test = df_test[['student_id', 'coach_id']].assign(relevance=1)\n",
    "\n",
    "# df_train = pd.concat([df_train, neg_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79799fa-f597-45cd-8774-a87710ff53c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Calculate statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935dac4c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Explore your data by calculating the number of unique users and movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f497c0de-10db-48a3-a3b6-9e74c6aaa79b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_unique_count(df: pd.DataFrame) -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"calculate unique user and movie counts\"\"\"\n",
    "    return df.student_id.nunique(), df.coach_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "cae1685f-0f28-46ea-b584-d0f2e555c51b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique number of users and movies in the whole dataset\n",
    "get_unique_count(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fdfbb782-813e-4b79-96ea-a52163a15a7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set shape: (4, 5)\n",
      "testing set shape: (4, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f'training set shape: {get_unique_count(df_train)}')\n",
    "print(f'testing set shape: {get_unique_count(df_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3b4ce-a518-411a-bd69-782fe1523d53",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, calculate the number of unique users and the number of movies in your training data for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "46c79269-24a6-4648-b677-6ff34eb3f261",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique users:  4\n",
      "number of unique items:  5\n"
     ]
    }
   ],
   "source": [
    "# number of unique user and number of unique item/movie\n",
    "n_user, n_item = get_unique_count(df_train)\n",
    "\n",
    "print('number of unique users: ', n_user)\n",
    "print('number of unique items: ', n_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d84fc-9591-4b93-9b7b-1de5b49f96ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Preprocess data and upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33463f2-b455-4273-8000-a5d2c3d55ae9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run the cells in this section to perform the training and testing splits, negative sampling, and store the processed data in [Amazon Simple Storage Service](https://aws.amazon.com/s3/) (Amazon S3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "72d34ba2-f9fd-442f-8615-3fe58690e392",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently in eu-central-1\n"
     ]
    }
   ],
   "source": [
    "# get current session region\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f'currently in {region}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea2ffb-4590-4a37-95ac-752ed9a60aa6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use the Amazon SageMaker session’s default bucket to store processed data. The format of the default bucket name is `sagemaker-{region}–{aws-account-id}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "20cb40b1-95d7-4598-a5bf-a8fd3a547cae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket name: sagemaker-eu-central-1-342301825291\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "print(f'bucket name: {bucket_name}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a328b3e-c32a-4ce4-b428-211a5566df52",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Upload your data to your Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f7bf8698-32be-4c45-8133-f8ba379910c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [2 1 0]\n",
      " [3 1 5]\n",
      " [4 1 5]\n",
      " [1 2 0]\n",
      " [2 2 5]\n",
      " [3 2 0]\n",
      " [4 2 0]\n",
      " [1 3 5]\n",
      " [2 3 1]\n",
      " [3 3 0]\n",
      " [4 3 0]\n",
      " [1 4 5]\n",
      " [2 4 4]\n",
      " [3 4 0]\n",
      " [4 4 0]\n",
      " [1 5 0]\n",
      " [2 5 2]\n",
      " [3 5 0]\n",
      " [4 5 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-central-1-342301825291/data/test.npy'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save data locally first\n",
    "dest = 'data/s3'\n",
    "train_path = os.path.join(dest, 'train.npy')\n",
    "test_path = os.path.join(dest, 'test.npy')\n",
    "\n",
    "# !mkdir {dest}\n",
    "np.save(train_path, df_train.values)\n",
    "print(df_train.values)\n",
    "np.save(test_path, df_test.values)\n",
    "\n",
    "# upload to S3 bucket (see the bucket name above)\n",
    "sagemaker_session.upload_data(train_path, key_prefix='data')\n",
    "sagemaker_session.upload_data(test_path, key_prefix='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97036551-c6fd-4453-bbc1-6e23f8352661",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Code NCF network in TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "65882570-ddf3-40e7-9d89-acad0be39325",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution role ARN: arn:aws:iam::342301825291:role/service-role/AmazonSageMaker-ExecutionRole-20230521T122971\n",
      "default bucket name: sagemaker-eu-central-1-342301825291\n"
     ]
    }
   ],
   "source": [
    "# import requirements\n",
    "import tensorflow as tf\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# get current SageMaker session's execution role and default bucket name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"execution role ARN:\", role)\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "print(\"default bucket name:\", bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4ee9f707-c717-477f-bff7-82892390df50",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# specify the location of the training data\n",
    "training_data_uri = os.path.join(f's3://{bucket_name}', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1f1515c7-0667-417c-9a50-1b7f4ea8f274",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\n",
      "\u001b[33m SPDX-License-Identifier: MIT-0\u001b[39;49;00m\n",
      "\u001b[33m \u001b[39;49;00m\n",
      "\u001b[33m Permission is hereby granted, free of charge, to any person obtaining a copy of this\u001b[39;49;00m\n",
      "\u001b[33m software and associated documentation files (the \"Software\"), to deal in the Software\u001b[39;49;00m\n",
      "\u001b[33m without restriction, including without limitation the rights to use, copy, modify,\u001b[39;49;00m\n",
      "\u001b[33m merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\u001b[39;49;00m\n",
      "\u001b[33m permit persons to whom the Software is furnished to do so.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\u001b[39;49;00m\n",
      "\u001b[33m INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\u001b[39;49;00m\n",
      "\u001b[33m PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\u001b[39;49;00m\n",
      "\u001b[33m HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\u001b[39;49;00m\n",
      "\u001b[33m OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\u001b[39;49;00m\n",
      "\u001b[33m SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# for data processing\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_training_data\u001b[39;49;00m(base_dir: \u001b[36mstr\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\" load training data \"\"\"\u001b[39;49;00m\n",
      "    df_train = np.load(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    user_train, item_train, y_train = np.split(np.transpose(df_train).flatten(), \u001b[34m3\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m user_train, item_train, y_train\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mbatch_generator\u001b[39;49;00m(x, y, batch_size, n_batch, shuffle, user_dim, item_dim):\n",
      "    \u001b[33m\"\"\" batch generator to supply data for training and testing \"\"\"\u001b[39;49;00m\n",
      "\n",
      "    user_df, item_df = x\n",
      "\n",
      "    counter = \u001b[34m0\u001b[39;49;00m\n",
      "    training_index = np.arange(user_df.shape[\u001b[34m0\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m shuffle:\n",
      "        np.random.shuffle(training_index)\n",
      "\n",
      "    \u001b[34mwhile\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m:\n",
      "        batch_index = training_index[batch_size*counter:batch_size*(counter+\u001b[34m1\u001b[39;49;00m)]\n",
      "        user_batch = tf.one_hot(user_df[batch_index], depth=user_dim)\n",
      "        item_batch = tf.one_hot(item_df[batch_index], depth=item_dim)\n",
      "        y_batch = y[batch_index]\n",
      "        counter += \u001b[34m1\u001b[39;49;00m\n",
      "        \u001b[34myield\u001b[39;49;00m [user_batch, item_batch], y_batch\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m counter == n_batch:\n",
      "            \u001b[34mif\u001b[39;49;00m shuffle:\n",
      "                np.random.shuffle(training_index)\n",
      "            counter = \u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# network\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_user_embedding_layers\u001b[39;49;00m(inputs, emb_dim):\n",
      "    \u001b[33m\"\"\" create user embeddings \"\"\"\u001b[39;49;00m\n",
      "    user_gmf_emb = tf.keras.layers.Dense(emb_dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs)\n",
      "    user_mlp_emb = tf.keras.layers.Dense(emb_dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m user_gmf_emb, user_mlp_emb\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_item_embedding_layers\u001b[39;49;00m(inputs, emb_dim):\n",
      "    \u001b[33m\"\"\" create item embeddings \"\"\"\u001b[39;49;00m\n",
      "    item_gmf_emb = tf.keras.layers.Dense(emb_dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs)\n",
      "    item_mlp_emb = tf.keras.layers.Dense(emb_dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m item_gmf_emb, item_mlp_emb\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_gmf\u001b[39;49;00m(user_emb, item_emb):\n",
      "    \u001b[33m\"\"\" general matrix factorization branch \"\"\"\u001b[39;49;00m\n",
      "    gmf_mat = tf.keras.layers.Multiply()([user_emb, item_emb])\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m gmf_mat\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_mlp\u001b[39;49;00m(user_emb, item_emb, dropout_rate):\n",
      "    \u001b[33m\"\"\" multi-layer perceptron branch \"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32madd_layer\u001b[39;49;00m(dim, input_layer, dropout_rate):\n",
      "        hidden_layer = tf.keras.layers.Dense(dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(input_layer)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m dropout_rate:\n",
      "            dropout_layer = tf.keras.layers.Dropout(dropout_rate)(hidden_layer)\n",
      "            \u001b[34mreturn\u001b[39;49;00m dropout_layer\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m hidden_layer\n",
      "\n",
      "    concat_layer = tf.keras.layers.Concatenate()([user_emb, item_emb])\n",
      "    dropout_l1 = tf.keras.layers.Dropout(dropout_rate)(concat_layer)\n",
      "    dense_layer_1 = add_layer(\u001b[34m64\u001b[39;49;00m, dropout_l1, dropout_rate)\n",
      "    dense_layer_2 = add_layer(\u001b[34m32\u001b[39;49;00m, dense_layer_1, dropout_rate)\n",
      "    dense_layer_3 = add_layer(\u001b[34m16\u001b[39;49;00m, dense_layer_2, \u001b[34mNone\u001b[39;49;00m)\n",
      "    dense_layer_4 = add_layer(\u001b[34m8\u001b[39;49;00m, dense_layer_3, \u001b[34mNone\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m dense_layer_4\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_neuCF\u001b[39;49;00m(gmf, mlp):\n",
      "    concat_layer = tf.keras.layers.Concatenate()([gmf, mlp])\n",
      "    output_layer = tf.keras.layers.Dense(\u001b[34m1\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msigmoid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(concat_layer)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m output_layer\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mbuild_graph\u001b[39;49;00m(user_dim, item_dim, dropout_rate=\u001b[34m0.25\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\" neural collaborative filtering model \"\"\"\u001b[39;49;00m\n",
      "\n",
      "    user_input = tf.keras.Input(shape=(user_dim))\n",
      "    item_input = tf.keras.Input(shape=(item_dim))\n",
      "\n",
      "    \u001b[37m# create embedding layers\u001b[39;49;00m\n",
      "    user_gmf_emb, user_mlp_emb = _get_user_embedding_layers(user_input, \u001b[34m32\u001b[39;49;00m)\n",
      "    item_gmf_emb, item_mlp_emb = _get_item_embedding_layers(item_input, \u001b[34m32\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# general matrix factorization\u001b[39;49;00m\n",
      "    gmf = _gmf(user_gmf_emb, item_gmf_emb)\n",
      "\n",
      "    \u001b[37m# multi layer perceptron\u001b[39;49;00m\n",
      "    mlp = _mlp(user_mlp_emb, item_mlp_emb, dropout_rate)\n",
      "\n",
      "    \u001b[37m# output\u001b[39;49;00m\n",
      "    output = _neuCF(gmf, mlp)\n",
      "\n",
      "    \u001b[37m# create the model\u001b[39;49;00m\n",
      "    model = tf.keras.Model(inputs=[user_input, item_input], outputs=output)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel\u001b[39;49;00m(x_train, y_train, n_user, n_item, num_epoch, batch_size):\n",
      "\n",
      "    num_batch = np.ceil(x_train[\u001b[34m0\u001b[39;49;00m].shape[\u001b[34m0\u001b[39;49;00m]/batch_size)\n",
      "\n",
      "    \u001b[37m# build graph\u001b[39;49;00m\n",
      "    model = build_graph(n_user, n_item)\n",
      "\n",
      "    \u001b[37m# compile and train\u001b[39;49;00m\n",
      "    optimizer = tf.keras.optimizers.Adam(learning_rate=\u001b[34m1e-3\u001b[39;49;00m)\n",
      "\n",
      "    model.compile(optimizer=optimizer,\n",
      "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
      "                  metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    model.fit_generator(\n",
      "        generator=batch_generator(\n",
      "            x=x_train, y=y_train,\n",
      "            batch_size=batch_size, n_batch=num_batch,\n",
      "            shuffle=\u001b[34mTrue\u001b[39;49;00m, user_dim=n_user, item_dim=n_item),\n",
      "        epochs=num_epoch,\n",
      "        steps_per_epoch=num_batch,\n",
      "        verbose=\u001b[34m2\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_user\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_item\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args, unknown = _parse_args()\n",
      "\n",
      "    \u001b[37m# load data\u001b[39;49;00m\n",
      "    user_train, item_train, train_labels = _load_training_data(args.train)\n",
      "\n",
      "    \u001b[37m# build model\u001b[39;49;00m\n",
      "    ncf_model = model(\n",
      "        x_train=[user_train, item_train],\n",
      "        y_train=train_labels,\n",
      "        n_user=args.n_user,\n",
      "        n_item=args.n_item,\n",
      "        num_epoch=args.epochs,\n",
      "        batch_size=args.batch_size\n",
      "    )\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.current_host == args.hosts[\u001b[34m0\u001b[39;49;00m]:\n",
      "        \u001b[37m# save model to an S3 directory with version number '00000001'\u001b[39;49;00m\n",
      "        ncf_model.save(os.path.join(args.sm_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33m000000001\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mneural_collaborative_filtering.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "# inspect the training script using pygmentize\n",
    "!pygmentize 'ncf.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea4fdf6-7334-4266-8474-4cedfc167274",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The output architecture should look like the following diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55628a-9345-48f1-b290-46e59b1df97a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"docs/keras-diagram.jpeg\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c6098-e7a7-4359-af9d-b9034eeb4986",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Perform model training using script mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead736d1-ba4b-40f3-8dc0-446d23651e50",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For more information on deploying a model you trained using an instance on Amazon SageMaker, see [Deploy trained Keras or TensorFlow models using Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/). This solution deploys the model using script mode.\n",
    "\n",
    "You first need to create a Python script that contains the model training code. The model architecture code is already available to you in the file `ncf.py`. This file also contains functions for you to load training and testing data. Run the following cell to define your training environment specifications and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5804bda6-4f3c-4514-a6d6-4bba0d58f174",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# specify training instance type and model hyperparameters - note that for demo purposes, the number of epochs is set to 1\n",
    "instance_count = 1                 # number of instances to use for training\n",
    "instance_type = 'ml.m5.2xlarge'    # type of instance to use for training\n",
    "\n",
    "training_script = 'ncf.py'\n",
    "\n",
    "training_parameters = {\n",
    "    'epochs': 1,\n",
    "    'batch_size': 256, \n",
    "    'n_user': n_user, \n",
    "    'n_item': n_item\n",
    "}\n",
    "\n",
    "# training framework specs\n",
    "tensorflow_version = '2.5'\n",
    "python_version = 'py37'\n",
    "distributed_training_spec = {'parameter_server': {'enabled': True}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56022619-d220-4e76-aabb-5999578cb54d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Initiate the training job using a TensorFlow estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5286d6ba-dfa6-4dd2-aa59-d7e2c7620b86",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s3_prefix = \"custom_recommender_system\"\n",
    "output_path = f\"s3://{bucket_name}/{s3_prefix}/output\"\n",
    "\n",
    "ncf_estimator = TensorFlow(\n",
    "    entry_point=training_script,\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    framework_version=tensorflow_version,\n",
    "    py_version=python_version,\n",
    "    distribution=distributed_training_spec,\n",
    "    hyperparameters=training_parameters,\n",
    "    output_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc206cf-0c7e-4144-a7b4-c2f9444f5c07",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Kick off the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b8d9a1b9-43d5-44d6-a30a-c89e30cce675",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-21 19:01:53 Starting - Starting the training job...\n",
      "2023-05-21 19:02:18 Starting - Preparing the instances for trainingProfilerReport-1684695713: InProgress\n",
      "......\n",
      "2023-05-21 19:03:18 Downloading - Downloading input data\n",
      "2023-05-21 19:03:18 Training - Downloading the training image...\n",
      "2023-05-21 19:03:38 Training - Training image download completed. Training in progress...\u001b[34m2023-05-21 19:03:58.319400: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-05-21 19:03:58.319550: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2023-05-21 19:03:58.345765: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-05-21 19:03:59,502 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2023-05-21 19:03:59,510 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-05-21 19:03:59,700 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-05-21 19:03:59,716 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-05-21 19:03:59,732 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-05-21 19:03:59,743 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_parameter_server_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 256,\n",
      "        \"epochs\": 1,\n",
      "        \"model_dir\": \"s3://sagemaker-eu-central-1-342301825291/custom_recommender_system/output/tensorflow-training-2023-05-21-19-01-53-052/model\",\n",
      "        \"n_item\": 5,\n",
      "        \"n_user\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"tensorflow-training-2023-05-21-19-01-53-052\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-342301825291/tensorflow-training-2023-05-21-19-01-53-052/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ncf\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ncf.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":256,\"epochs\":1,\"model_dir\":\"s3://sagemaker-eu-central-1-342301825291/custom_recommender_system/output/tensorflow-training-2023-05-21-19-01-53-052/model\",\"n_item\":5,\"n_user\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=ncf.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=ncf\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-342301825291/tensorflow-training-2023-05-21-19-01-53-052/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":256,\"epochs\":1,\"model_dir\":\"s3://sagemaker-eu-central-1-342301825291/custom_recommender_system/output/tensorflow-training-2023-05-21-19-01-53-052/model\",\"n_item\":5,\"n_user\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"tensorflow-training-2023-05-21-19-01-53-052\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-342301825291/tensorflow-training-2023-05-21-19-01-53-052/source/sourcedir.tar.gz\",\"module_name\":\"ncf\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ncf.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"256\",\"--epochs\",\"1\",\"--model_dir\",\"s3://sagemaker-eu-central-1-342301825291/custom_recommender_system/output/tensorflow-training-2023-05-21-19-01-53-052/model\",\"--n_item\",\"5\",\"--n_user\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-eu-central-1-342301825291/custom_recommender_system/output/tensorflow-training-2023-05-21-19-01-53-052/model\u001b[0m\n",
      "\u001b[34mSM_HP_N_ITEM=5\u001b[0m\n",
      "\u001b[34mSM_HP_N_USER=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 ncf.py --batch_size 256 --epochs 1 --model_dir s3://sagemaker-eu-central-1-342301825291/custom_recommender_system/output/tensorflow-training-2023-05-21-19-01-53-052/model --n_item 5 --n_user 4\u001b[0m\n",
      "\u001b[34m2023-05-21 19:04:00.243463: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-05-21 19:04:00.243607: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2023-05-21 19:04:00.270221: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-21 19:04:01.819 ip-10-0-255-89.eu-central-1.compute.internal:37 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-21 19:04:01.887 ip-10-0-255-89.eu-central-1.compute.internal:37 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1991: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\u001b[0m\n",
      "\u001b[34m1/1 - 1s - loss: 0.6199 - accuracy: 0.1500\u001b[0m\n",
      "\u001b[34m2023-05-21 19:04:03.298083: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/000000001/assets\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/000000001/assets\u001b[0m\n",
      "\u001b[34m2023-05-21 19:04:04,650 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-21 19:04:04,650 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-21 19:04:04,650 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-21 19:04:18 Uploading - Uploading generated training model\n",
      "2023-05-21 19:04:18 Completed - Training job completed\n",
      "Training seconds: 81\n",
      "Billable seconds: 81\n"
     ]
    }
   ],
   "source": [
    "ncf_estimator.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e016ad5-99a2-488d-86dd-8c69b76e99bc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Deploy the trained model using Amazon SageMaker hosting services as an endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5c1d8-2c49-4d47-8361-2e7a43a3cd77",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After the model is trained, you can deploy the model using Amazon SageMaker Hosting Services. Here you deploy the model using one `ml.c5.xlarge` instance as a `tensorflow-serving` endpoint. This allows you to later invoke the endpoint like you would invoke Tensorflow Serving. For more information, see [Train and serve a TensorFlow model with TensorFlow Serving](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6f917041-b2ce-4755-b828-1a03318e3731",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Using already existing model: sagemaker-soln-crs-js-xd5uponeural-collab-filtering-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = sagemaker_config[\"SolutionPrefix\"] + \"neural-collab-filtering-endpoint\"\n",
    "model_name = sagemaker_config[\"SolutionPrefix\"] + \"neural-collab-filtering-model\"\n",
    "\n",
    "predictor = ncf_estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=\"ml.t2.medium\", \n",
    "    endpoint_name=endpoint_name,\n",
    "    model_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46a10a-6a70-49e3-b711-4f9b61da42da",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run inference using the model endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38adbaf3-4600-4338-a0e7-3d8775e4f9f4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To run inference using the endpoint on the testing set, invoke the model using [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5ea6ed69-f90a-40cb-9eaa-d1699c2a89ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [2 5 2]\n",
      " [3 2 0]\n",
      " [4 3 0]]\n"
     ]
    }
   ],
   "source": [
    "# to use the endpoint in another notebook, initiate a predictor object as follows\n",
    "from sagemaker.tensorflow import TensorFlowPredictor\n",
    "\n",
    "predictor = TensorFlowPredictor(endpoint_name)\n",
    "\n",
    "# define a function to read testing data\n",
    "def _load_testing_data(base_dir: str) -> Tuple[np.array, np.array, np.array]:\n",
    "    \"\"\" load testing data \"\"\"\n",
    "    df_test = np.load(os.path.join(base_dir, 'test.npy'))\n",
    "    df_test = np.delete(df_test, 0, 1)\n",
    "    print(df_test)\n",
    "    user_test, item_test, y_test = np.split(np.transpose(df_test).flatten(), 3)\n",
    "    return user_test, item_test, y_test\n",
    "\n",
    "# read testing data from local\n",
    "user_test, item_test, _ = _load_testing_data('./data/s3/')\n",
    "\n",
    "# one-hot encode the testing data for model input\n",
    "with tf.compat.v1.Session() as tf_sess:\n",
    "    test_user_data = tf_sess.run(tf.one_hot(user_test, depth=n_user)).tolist()\n",
    "    test_item_data = tf_sess.run(tf.one_hot(item_test, depth=n_item)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a5c754bb-5b61-444e-9128-b248d916bf21",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0]]\n",
      "[[0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0]]\n",
      "{'instances': [{'input_1': [0.0, 1.0, 0.0, 0.0], 'input_2': [0.0, 1.0, 0.0, 0.0, 0.0]}, {'input_1': [0.0, 0.0, 1.0, 0.0], 'input_2': [0.0, 0.0, 0.0, 0.0, 0.0]}, {'input_1': [0.0, 0.0, 0.0, 1.0], 'input_2': [0.0, 0.0, 1.0, 0.0, 0.0]}, {'input_1': [0.0, 0.0, 0.0, 0.0], 'input_2': [0.0, 0.0, 0.0, 1.0, 0.0]}]}\n",
      "{'predictions': [[0.509837568], [0.499842048], [0.516042471], [0.502878308]]}\n"
     ]
    }
   ],
   "source": [
    "# make batch prediction\n",
    "batch_size = 100\n",
    "y_pred = []\n",
    "print(test_user_data)\n",
    "print(test_item_data)\n",
    "for idx in range(0, len(test_user_data), batch_size):\n",
    "    # reformat test samples into a format acceptable to tensorflow serving\n",
    "    input_vals = {\n",
    "     \"instances\": [\n",
    "         {'input_1': u, 'input_2': i} \n",
    "         for (u, i) in zip(test_user_data[idx:idx+batch_size], test_item_data[idx:idx+batch_size])\n",
    "    ]}\n",
    " \n",
    "    print(input_vals)\n",
    "\n",
    "    # invoke model endpoint to run inference\n",
    "    pred = predictor.predict(input_vals)\n",
    "    \n",
    "    print(pred)\n",
    "    \n",
    "    # store predictions\n",
    "    y_pred.extend([i[0] for i in pred['predictions']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c328d-6088-4ae7-adf1-df909840f340",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The model output is a set of probabilities, ranging from 0 to 1, for each user-item pair that is specified for inference. To make final binary predictions, such as \"like\" or \"dislike\", you must apply a threshold. For demonstration purposes, this solution uses 0.5 as a threshold. If the predicted probability is equal to or greater than 0.5, then it is assumed that the user likes the movie. If the probability is less than 0.5, then it is assumed that the user dislikes the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272a37e-34d3-4ba2-be69-0376559feb95",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# let's see some prediction examples, assuming the threshold \n",
    "# --- prediction probability view ---\n",
    "print('This is what the prediction output looks like')\n",
    "print(y_pred[:5], end='\\n\\n\\n')\n",
    "\n",
    "# --- user item pair prediction view, with threshold of 0.5 applied ---\n",
    "pred_df = pd.DataFrame([\n",
    "    user_test,\n",
    "    item_test,\n",
    "    (np.array(y_pred) >= 0.5).astype(int)],\n",
    ").T\n",
    "\n",
    "pred_df.columns = ['userId', 'movieId', 'prediction']\n",
    "\n",
    "print('We can convert the output to user-item pair as shown below')\n",
    "print(pred_df.head(), end='\\n\\n\\n')\n",
    "\n",
    "# --- aggregated prediction view, by user ---\n",
    "print('Lastly, we can roll up the prediction list by user and view it that way')\n",
    "print(pred_df.query('prediction == 1').groupby('userId').movieId.apply(list).head().to_frame(), end='\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28fa976-cf42-43d9-b859-6727fb3e9e07",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e781f48-4527-40d2-bc4c-2791736d0b41",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Designing a recommender system can be a challenging task that sometimes requires model customization. In this solution, you implemented, deployed, and invoked an NCF model from scratch in Amazon SageMaker. This work can serve as a foundation for you to start building more customized solutions with your own datasets.\n",
    "\n",
    "For more information about using built-in Amazon SageMaker algorithms and [Amazon Personalize](https://aws.amazon.com/personalize/) to build recommender system solutions, see the following blog posts:\n",
    "\n",
    " - [Omnichannel personalization with Amazon Personalize](https://aws.amazon.com/blogs/machine-learning/omnichannel-personalization-with-amazon-personalize/)\n",
    " - [Creating a recommendation engine using Amazon Personalize](https://aws.amazon.com/blogs/machine-learning/creating-a-recommendation-engine-using-amazon-personalize/)\n",
    " - [Extending Amazon SageMaker factorization machines algorithms to predict top x recommendations](https://aws.amazon.com/blogs/machine-learning/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations/)\n",
    " - [Build a movie recommender with factorization machines on Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/)\n",
    " \n",
    "You can further customize the Neural Collaborative Filtering network using Deep Matrix Factorization (Xue et al., 2017)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7ec9d-0d75-43cb-8d09-d2324a0c5e99",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Delete endpoint (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2264ef-9f29-4828-82fe-4e385ed43d7a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After you complete this solution, remove the endpoint when it is no longer needed to save you from incurring further hosting charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0111952-7a2f-44a8-9389-852c6f09b5d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
